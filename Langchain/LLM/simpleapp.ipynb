{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7aa37d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"]=os.getenv(\"LANGSMITH_ENDPOINT\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf8f420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "## Data Ingestion -from website\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bbff41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x1a2929a8910>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=WebBaseLoader(\"https://docs.langchain.com/langsmith/evaluation-concepts\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62c70ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='Evaluation concepts - Docs by LangChainSkip to main contentDocs by LangChain home pageLangSmithSearch...⌘KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationEvaluation conceptsGet startedObservabilityEvaluationPrompt engineeringDeploymentPlatform setupReferenceOverviewQuickstartConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsCustom output renderingSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOn this pageWhat to evaluateOffline and online evaluationsOffline evaluationsOnline evaluationsEvaluation lifecycle1. Development with offline evaluation2. Initial deployment with online evaluation3. Continuous improvementCore evaluation targetsTargets for offline evaluationDatasetsExamplesExperimentTargets for online evaluationRunsThreadsEvaluatorsEvaluator inputsEvaluator outputsEvaluation techniquesHumanCodeLLM-as-judgePairwiseReference-free vs reference-based evaluatorsEvaluation typesBest practicesBuilding datasetsDataset organizationHuman feedback collectionEvaluations vs testingQuick reference: Offline vs online evaluationEvaluation conceptsCopy pageCopy pageLLM outputs are non-deterministic, which makes response quality hard to assess. Evaluations (evals) are a way to breakdown what “good” looks like and measure it. LangSmith Evaluation provides a framework for measuring quality throughout the application lifecycle, from pre-deployment testing to production monitoring.\\n\\u200bWhat to evaluate\\nBefore building evaluations, identify what matters for your application. Break down your system into its critical components—LLM calls, retrieval steps, tool invocations, output formatting—and determine quality criteria for each.\\nStart with manually curated examples. Create 5-10 examples of what “good” looks like for each critical component. These examples serve as your ground truth and inform which evaluation approaches to use. For instance:\\n\\nRAG system: Examples of good retrievals (relevant documents) and good answers (accurate, complete).\\nAgent: Examples of correct tool selection and proper argument formatting or trajectory that the agent took.\\nChatbot: Examples of helpful, on-brand responses that address user intent.\\n\\nOnce you’ve defined “good” through examples, you can measure how often your system produces similar quality outputs.\\n\\u200bOffline and online evaluations\\nLangSmith supports two types of evaluations that serve different purposes in your development workflow:\\n\\u200bOffline evaluations\\n Use offline evaluations for pre-deployment testing:\\n\\nBenchmarking: Compare multiple versions to find the best performer.\\nRegression testing: Ensure new versions don’t degrade quality.\\nUnit testing: Verify correctness of individual components.\\nBacktesting: Test new versions against historical data.\\n\\nOffline evaluations target examples from datasets—curated test cases with reference outputs that define what “good” looks like.\\n\\u200bOnline evaluations\\n Use online evaluations for production monitoring:\\n\\nReal-time monitoring: Track quality continuously on live traffic.\\nAnomaly detection: Flag unusual patterns or edge cases.\\nProduction feedback: Identify issues to add to offline datasets.\\n\\nOnline evaluations target runs and threads from tracing—real production traces without reference outputs.\\nThis difference in targets determines what you can evaluate: offline evaluations can check correctness against expected answers, while online evaluations focus on quality patterns, safety, and real-world behavior.\\n\\u200bEvaluation lifecycle\\nAs you develop and deploy your application, your evaluation strategy evolves from pre-deployment testing to production monitoring. During development and testing, offline evaluations validate functionality against curated datasets. After deployment, online evaluations monitor production behavior on live traffic. As applications mature, both evaluation types work together in an iterative feedback loop to improve quality continuously.\\n\\n\\u200b1. Development with offline evaluation\\nBefore production deployment, use offline evaluations to validate functionality, benchmark different approaches, and build confidence.\\nFollow the quickstart to run your first offline evaluation.\\n\\u200b2. Initial deployment with online evaluation\\nAfter deployment, use online evaluations to monitor production quality, detect unexpected issues, and collect real-world data.\\nLearn how to configure online evaluations for production monitoring.\\n\\u200b3. Continuous improvement\\nUse both evaluation types together in an iterative feedback loop. Online evaluations surface issues that become offline test cases, offline evaluations validate fixes, and online evaluations confirm production improvements.\\n\\u200bCore evaluation targets\\nEvaluations run on different targets depending on whether they are offline or online.\\n\\u200bTargets for offline evaluation\\nOffline evaluations run on datasets and examples. The presence of reference outputs enables comparison between expected and actual results.\\n\\u200bDatasets\\nA dataset is a collection of examples used for evaluating an application. An example is a test input, reference output pair.\\n\\n\\u200bExamples\\nEach example consists of:\\n\\nInputs: a dictionary of input variables to pass to your application.\\nReference outputs (optional): a dictionary of reference outputs. These do not get passed to your application, they are only used in evaluators.\\nMetadata (optional): a dictionary of additional information that can be used to create filtered views of a dataset.\\n\\n\\nLearn more about managing datasets.\\n\\u200bExperiment\\nAn experiment represents the results of evaluating a specific application version on a dataset. Each experiment captures outputs, evaluator scores, and execution traces for every example in the dataset.\\n\\nMultiple experiments typically run on a given dataset to test different application configurations (e.g., different prompts or LLMs). LangSmith displays all experiments associated with a dataset and supports comparing multiple experiments side-by-side.\\n\\nLearn how to analyze experiment results.\\n\\u200bTargets for online evaluation\\nOnline evaluations run on runs and threads from production traffic. Without reference outputs, evaluators focus on detecting issues, anomalies, and quality degradation in real-time.\\n\\u200bRuns\\nA run is a single execution trace from your deployed application. Each run contains:\\n\\nInputs: The actual user inputs your application received.\\nOutputs: What your application actually returned.\\nIntermediate steps: All the child runs (tool calls, LLM calls, and so on).\\nMetadata: Tags, user feedback, latency metrics, etc.\\n\\nUnlike examples in datasets, runs do not include reference outputs. Online evaluators must assess quality without knowing what the “correct” answer should be, relying instead on quality heuristics, safety checks, and reference-free evaluation techniques.\\nLearn more about runs and traces in the Observability concepts.\\n\\u200bThreads\\nThreads are collections of related runs representing multi-turn conversations. Online evaluators can run at the thread level to evaluate entire conversations rather than individual turns. This enables assessment of conversation-level properties like coherence across turns, topic maintenance, and user satisfaction throughout an interaction.\\n\\u200bEvaluators\\nEvaluators are functions that score application performance. They provide the measurement layer for both offline and online evaluation, adapting their inputs based on what data is available.\\nRun evaluators using the LangSmith SDK (Python and TypeScript), via the Prompt Playground, or by configuring rules to run them automatically on tracing projects or datasets.\\n\\u200bEvaluator inputs\\nEvaluator inputs differ based on evaluation type:\\nOffline evaluators receive:\\n\\nExample: The example from your dataset, containing inputs, reference outputs, and metadata.\\nRun: The actual outputs and intermediate steps from running the application on the example inputs.\\n\\nOnline evaluators receive:\\n\\nRun: The production trace containing inputs, outputs, and intermediate steps (no reference outputs available).\\n\\n\\u200bEvaluator outputs\\nEvaluators return feedback, which is the scores from evaluation. Feedback is a dictionary or list of dictionaries. Each dictionary contains:\\n\\nkey: The metric name.\\nscore | value: The metric value (score for numerical metrics, value for categorical metrics).\\ncomment (optional): Additional reasoning or explanation for the score.\\n\\n\\u200bEvaluation techniques\\nLangSmith supports several evaluation approaches:\\n\\nHuman\\nCode\\nLLM-as-judge\\nPairwise\\n\\n\\u200bHuman\\nHuman evaluation involves manual review of application outputs and execution traces. This approach is often an effective starting point for evaluation. LangSmith provides tools to review application outputs and traces (all intermediate steps).\\nAnnotation queues streamline the process of collecting human feedback on application outputs.\\n\\u200bCode\\nCode evaluators are deterministic, rule-based functions. They work well for checks such as verifying the structure of a chatbot’s response is not empty, that generated code compiles, or that a classification matches exactly.\\n\\u200bLLM-as-judge\\nLLM-as-judge evaluators use LLMs to score application outputs. The grading rules and criteria are typically encoded in the LLM prompt. These evaluators can be:\\n\\nReference-free: Check if output contains offensive content or adheres to specific criteria.\\nReference-based: Compare output to a reference (e.g., check factual accuracy relative to the reference).\\n\\nLLM-as-judge evaluators require careful review of scores and prompt tuning. Few-shot evaluators, which include examples of inputs, outputs, and expected grades in the grader prompt, often improve performance.\\nLearn about how to define an LLM-as-a-judge evaluator.\\n\\u200bPairwise\\nPairwise evaluators compare outputs from two application versions using heuristics (e.g., which response is longer), LLMs (with pairwise prompts), or human reviewers.\\nPairwise evaluation works well when directly scoring an output is difficult but comparing two outputs is straightforward. For example, in summarization tasks, choosing the more informative of two summaries is often easier than assigning an absolute score to a single summary.\\nLearn how run pairwise evaluations.\\n\\u200bReference-free vs reference-based evaluators\\nUnderstanding whether an evaluator requires reference outputs is essential for determining when it can be used.\\nReference-free evaluators assess quality without comparing to expected outputs. These work for both offline and online evaluation:\\n\\nSafety checks: Toxicity detection, PII detection, content policy violations\\nFormat validation: JSON structure, required fields, schema compliance\\nQuality heuristics: Response length, latency, specific keywords\\nReference-free LLM-as-judge: Clarity, coherence, helpfulness, tone\\n\\nReference-based evaluators require reference outputs and only work for offline evaluation:\\n\\nCorrectness: Semantic similarity to reference answer\\nFactual accuracy: Fact-checking against ground truth\\nExact match: Classification tasks with known labels\\nReference-based LLM-as-judge: Comparing output quality to a reference\\n\\nWhen designing an evaluation strategy, reference-free evaluators provide consistency across both offline testing and online monitoring, while reference-based evaluators enable more precise correctness checks during development.\\n\\u200bEvaluation types\\nLangSmith supports various evaluation approaches for different stages of development and deployment. Understanding when to use each type helps build a comprehensive evaluation strategy.\\nOffline and online evaluations serve different purposes:\\n\\nOffline evaluation types test pre-deployment on curated datasets with reference outputs\\nOnline evaluation types monitor production behavior on live traffic without reference outputs\\n\\nLearn more about evaluation types and when to use each.\\n\\u200bBest practices\\n\\u200bBuilding datasets\\nThere are various strategies for building datasets:\\nManually curated examples\\nThis is the recommended starting point. Create 10–20 high-quality examples covering common scenarios and edge cases. These examples define what “good” looks like for your application.\\nHistorical traces\\nOnce in production, convert real traces into examples. For high-traffic applications:\\n\\nUser feedback: Add runs that received negative feedback to test against.\\nHeuristics: Identify interesting runs (e.g., long latency, errors).\\nLLM feedback: Use LLMs to detect noteworthy conversations.\\n\\nSynthetic data\\nGenerate additional examples from existing ones. Works best when starting with several high-quality, hand-crafted examples as templates.\\n\\u200bDataset organization\\nSplits\\nPartition datasets into subsets for targeted evaluation. Use splits for performance optimization (smaller splits for rapid iteration) and interpretability (evaluate different input types separately).\\nLearn how to create and manage dataset splits.\\nVersions\\nLangSmith automatically creates dataset versions when examples change. Tag versions to mark important milestones. Target specific versions in CI pipelines to ensure dataset updates don’t break workflows.\\n\\u200bHuman feedback collection\\nHuman feedback often provides the most valuable assessment, particularly for subjective quality dimensions.\\nAnnotation queues\\nAnnotation queues enable structured collection of human feedback. Flag specific runs for review, collect annotations in a streamlined interface, and transfer annotated runs to datasets for future evaluations.\\nAnnotation queues complement inline annotation by offering additional capabilities: grouping runs, specifying criteria, and configuring reviewer permissions.\\n\\u200bEvaluations vs testing\\nTesting and evaluation are similar but distinct concepts.\\nEvaluation measures performance according to metrics. Metrics can be fuzzy or subjective, and prove more useful in relative terms. They typically compare systems against each other.\\nTesting asserts correctness. A system can only be deployed if it passes all tests.\\nEvaluation metrics can be converted into tests. For example, regression tests can assert that new versions must outperform baseline versions on relevant metrics. Run tests and evaluations together for efficiency when systems are expensive to run.\\nEvaluations can be written using standard testing tools like pytest or Vitest/Jest.\\n\\u200bQuick reference: Offline vs online evaluation\\nThe following table summarizes the key differences between offline and online evaluations:\\nOffline EvaluationOnline EvaluationRuns onDataset (Examples)Tracing Project (Runs/Threads)Data accessInputs, Outputs, Reference OutputsInputs, Outputs onlyWhen to usePre-deployment, during developmentProduction, post-deploymentPrimary use casesBenchmarking, unit testing, regression testing, backtestingReal-time monitoring, production feedback, anomaly detectionEvaluation timingBatch processing on curated test setsReal-time or near real-time on live trafficSetup locationEvaluation tab (SDK, UI, Prompt Playground)Observability tab (automated rules)Data requirementsRequires dataset curationNo dataset needed, evaluates live traces\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoEvaluation quickstartPreviousApplication-specific evaluation approachesNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by\\n')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=docs.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20bbbd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Data --> Docs --> Divide our text into chunks --> textVectors --> Vector Embedding --> vector Storing\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "document = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "742e3745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='Evaluation concepts - Docs by LangChainSkip to main contentDocs by LangChain home pageLangSmithSearch...⌘KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationEvaluation conceptsGet startedObservabilityEvaluationPrompt engineeringDeploymentPlatform setupReferenceOverviewQuickstartConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsCustom output renderingSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOn this pageWhat to evaluateOffline and online evaluationsOffline evaluationsOnline'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOn this pageWhat to evaluateOffline and online evaluationsOffline evaluationsOnline evaluationsEvaluation lifecycle1. Development with offline evaluation2. Initial deployment with online evaluation3. Continuous improvementCore evaluation targetsTargets for offline evaluationDatasetsExamplesExperimentTargets for online evaluationRunsThreadsEvaluatorsEvaluator inputsEvaluator outputsEvaluation techniquesHumanCodeLLM-as-judgePairwiseReference-free vs reference-based evaluatorsEvaluation typesBest practicesBuilding datasetsDataset organizationHuman feedback collectionEvaluations vs testingQuick reference: Offline vs online evaluationEvaluation conceptsCopy pageCopy pageLLM outputs are non-deterministic, which makes response quality hard to assess. Evaluations (evals) are a way to breakdown what “good” looks like and measure it. LangSmith Evaluation provides a framework for'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='are non-deterministic, which makes response quality hard to assess. Evaluations (evals) are a way to breakdown what “good” looks like and measure it. LangSmith Evaluation provides a framework for measuring quality throughout the application lifecycle, from pre-deployment testing to production monitoring.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='\\u200bWhat to evaluate\\nBefore building evaluations, identify what matters for your application. Break down your system into its critical components—LLM calls, retrieval steps, tool invocations, output formatting—and determine quality criteria for each.\\nStart with manually curated examples. Create 5-10 examples of what “good” looks like for each critical component. These examples serve as your ground truth and inform which evaluation approaches to use. For instance:'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='RAG system: Examples of good retrievals (relevant documents) and good answers (accurate, complete).\\nAgent: Examples of correct tool selection and proper argument formatting or trajectory that the agent took.\\nChatbot: Examples of helpful, on-brand responses that address user intent.\\n\\nOnce you’ve defined “good” through examples, you can measure how often your system produces similar quality outputs.\\n\\u200bOffline and online evaluations\\nLangSmith supports two types of evaluations that serve different purposes in your development workflow:\\n\\u200bOffline evaluations\\n Use offline evaluations for pre-deployment testing:\\n\\nBenchmarking: Compare multiple versions to find the best performer.\\nRegression testing: Ensure new versions don’t degrade quality.\\nUnit testing: Verify correctness of individual components.\\nBacktesting: Test new versions against historical data.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='Offline evaluations target examples from datasets—curated test cases with reference outputs that define what “good” looks like.\\n\\u200bOnline evaluations\\n Use online evaluations for production monitoring:\\n\\nReal-time monitoring: Track quality continuously on live traffic.\\nAnomaly detection: Flag unusual patterns or edge cases.\\nProduction feedback: Identify issues to add to offline datasets.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='Real-time monitoring: Track quality continuously on live traffic.\\nAnomaly detection: Flag unusual patterns or edge cases.\\nProduction feedback: Identify issues to add to offline datasets.\\n\\nOnline evaluations target runs and threads from tracing—real production traces without reference outputs.\\nThis difference in targets determines what you can evaluate: offline evaluations can check correctness against expected answers, while online evaluations focus on quality patterns, safety, and real-world behavior.\\n\\u200bEvaluation lifecycle\\nAs you develop and deploy your application, your evaluation strategy evolves from pre-deployment testing to production monitoring. During development and testing, offline evaluations validate functionality against curated datasets. After deployment, online evaluations monitor production behavior on live traffic. As applications mature, both evaluation types work together in an iterative feedback loop to improve quality continuously.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='\\u200b1. Development with offline evaluation\\nBefore production deployment, use offline evaluations to validate functionality, benchmark different approaches, and build confidence.\\nFollow the quickstart to run your first offline evaluation.\\n\\u200b2. Initial deployment with online evaluation\\nAfter deployment, use online evaluations to monitor production quality, detect unexpected issues, and collect real-world data.\\nLearn how to configure online evaluations for production monitoring.\\n\\u200b3. Continuous improvement\\nUse both evaluation types together in an iterative feedback loop. Online evaluations surface issues that become offline test cases, offline evaluations validate fixes, and online evaluations confirm production improvements.\\n\\u200bCore evaluation targets\\nEvaluations run on different targets depending on whether they are offline or online.\\n\\u200bTargets for offline evaluation'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='\\u200bCore evaluation targets\\nEvaluations run on different targets depending on whether they are offline or online.\\n\\u200bTargets for offline evaluation\\nOffline evaluations run on datasets and examples. The presence of reference outputs enables comparison between expected and actual results.\\n\\u200bDatasets\\nA dataset is a collection of examples used for evaluating an application. An example is a test input, reference output pair.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='\\u200bExamples\\nEach example consists of:\\n\\nInputs: a dictionary of input variables to pass to your application.\\nReference outputs (optional): a dictionary of reference outputs. These do not get passed to your application, they are only used in evaluators.\\nMetadata (optional): a dictionary of additional information that can be used to create filtered views of a dataset.\\n\\n\\nLearn more about managing datasets.\\n\\u200bExperiment\\nAn experiment represents the results of evaluating a specific application version on a dataset. Each experiment captures outputs, evaluator scores, and execution traces for every example in the dataset.\\n\\nMultiple experiments typically run on a given dataset to test different application configurations (e.g., different prompts or LLMs). LangSmith displays all experiments associated with a dataset and supports comparing multiple experiments side-by-side.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='Learn how to analyze experiment results.\\n\\u200bTargets for online evaluation\\nOnline evaluations run on runs and threads from production traffic. Without reference outputs, evaluators focus on detecting issues, anomalies, and quality degradation in real-time.\\n\\u200bRuns\\nA run is a single execution trace from your deployed application. Each run contains:\\n\\nInputs: The actual user inputs your application received.\\nOutputs: What your application actually returned.\\nIntermediate steps: All the child runs (tool calls, LLM calls, and so on).\\nMetadata: Tags, user feedback, latency metrics, etc.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='Unlike examples in datasets, runs do not include reference outputs. Online evaluators must assess quality without knowing what the “correct” answer should be, relying instead on quality heuristics, safety checks, and reference-free evaluation techniques.\\nLearn more about runs and traces in the Observability concepts.\\n\\u200bThreads\\nThreads are collections of related runs representing multi-turn conversations. Online evaluators can run at the thread level to evaluate entire conversations rather than individual turns. This enables assessment of conversation-level properties like coherence across turns, topic maintenance, and user satisfaction throughout an interaction.\\n\\u200bEvaluators\\nEvaluators are functions that score application performance. They provide the measurement layer for both offline and online evaluation, adapting their inputs based on what data is available.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='Evaluators are functions that score application performance. They provide the measurement layer for both offline and online evaluation, adapting their inputs based on what data is available.\\nRun evaluators using the LangSmith SDK (Python and TypeScript), via the Prompt Playground, or by configuring rules to run them automatically on tracing projects or datasets.\\n\\u200bEvaluator inputs\\nEvaluator inputs differ based on evaluation type:\\nOffline evaluators receive:'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='Example: The example from your dataset, containing inputs, reference outputs, and metadata.\\nRun: The actual outputs and intermediate steps from running the application on the example inputs.\\n\\nOnline evaluators receive:\\n\\nRun: The production trace containing inputs, outputs, and intermediate steps (no reference outputs available).\\n\\n\\u200bEvaluator outputs\\nEvaluators return feedback, which is the scores from evaluation. Feedback is a dictionary or list of dictionaries. Each dictionary contains:\\n\\nkey: The metric name.\\nscore | value: The metric value (score for numerical metrics, value for categorical metrics).\\ncomment (optional): Additional reasoning or explanation for the score.\\n\\n\\u200bEvaluation techniques\\nLangSmith supports several evaluation approaches:\\n\\nHuman\\nCode\\nLLM-as-judge\\nPairwise'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='\\u200bEvaluation techniques\\nLangSmith supports several evaluation approaches:\\n\\nHuman\\nCode\\nLLM-as-judge\\nPairwise\\n\\n\\u200bHuman\\nHuman evaluation involves manual review of application outputs and execution traces. This approach is often an effective starting point for evaluation. LangSmith provides tools to review application outputs and traces (all intermediate steps).\\nAnnotation queues streamline the process of collecting human feedback on application outputs.\\n\\u200bCode\\nCode evaluators are deterministic, rule-based functions. They work well for checks such as verifying the structure of a chatbot’s response is not empty, that generated code compiles, or that a classification matches exactly.\\n\\u200bLLM-as-judge\\nLLM-as-judge evaluators use LLMs to score application outputs. The grading rules and criteria are typically encoded in the LLM prompt. These evaluators can be:'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='Reference-free: Check if output contains offensive content or adheres to specific criteria.\\nReference-based: Compare output to a reference (e.g., check factual accuracy relative to the reference).'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='LLM-as-judge evaluators require careful review of scores and prompt tuning. Few-shot evaluators, which include examples of inputs, outputs, and expected grades in the grader prompt, often improve performance.\\nLearn about how to define an LLM-as-a-judge evaluator.\\n\\u200bPairwise\\nPairwise evaluators compare outputs from two application versions using heuristics (e.g., which response is longer), LLMs (with pairwise prompts), or human reviewers.\\nPairwise evaluation works well when directly scoring an output is difficult but comparing two outputs is straightforward. For example, in summarization tasks, choosing the more informative of two summaries is often easier than assigning an absolute score to a single summary.\\nLearn how run pairwise evaluations.\\n\\u200bReference-free vs reference-based evaluators\\nUnderstanding whether an evaluator requires reference outputs is essential for determining when it can be used.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='Learn how run pairwise evaluations.\\n\\u200bReference-free vs reference-based evaluators\\nUnderstanding whether an evaluator requires reference outputs is essential for determining when it can be used.\\nReference-free evaluators assess quality without comparing to expected outputs. These work for both offline and online evaluation:'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='Safety checks: Toxicity detection, PII detection, content policy violations\\nFormat validation: JSON structure, required fields, schema compliance\\nQuality heuristics: Response length, latency, specific keywords\\nReference-free LLM-as-judge: Clarity, coherence, helpfulness, tone\\n\\nReference-based evaluators require reference outputs and only work for offline evaluation:\\n\\nCorrectness: Semantic similarity to reference answer\\nFactual accuracy: Fact-checking against ground truth\\nExact match: Classification tasks with known labels\\nReference-based LLM-as-judge: Comparing output quality to a reference'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='When designing an evaluation strategy, reference-free evaluators provide consistency across both offline testing and online monitoring, while reference-based evaluators enable more precise correctness checks during development.\\n\\u200bEvaluation types\\nLangSmith supports various evaluation approaches for different stages of development and deployment. Understanding when to use each type helps build a comprehensive evaluation strategy.\\nOffline and online evaluations serve different purposes:\\n\\nOffline evaluation types test pre-deployment on curated datasets with reference outputs\\nOnline evaluation types monitor production behavior on live traffic without reference outputs'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='Offline evaluation types test pre-deployment on curated datasets with reference outputs\\nOnline evaluation types monitor production behavior on live traffic without reference outputs\\n\\nLearn more about evaluation types and when to use each.\\n\\u200bBest practices\\n\\u200bBuilding datasets\\nThere are various strategies for building datasets:\\nManually curated examples\\nThis is the recommended starting point. Create 10–20 high-quality examples covering common scenarios and edge cases. These examples define what “good” looks like for your application.\\nHistorical traces\\nOnce in production, convert real traces into examples. For high-traffic applications:\\n\\nUser feedback: Add runs that received negative feedback to test against.\\nHeuristics: Identify interesting runs (e.g., long latency, errors).\\nLLM feedback: Use LLMs to detect noteworthy conversations.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='Synthetic data\\nGenerate additional examples from existing ones. Works best when starting with several high-quality, hand-crafted examples as templates.\\n\\u200bDataset organization\\nSplits\\nPartition datasets into subsets for targeted evaluation. Use splits for performance optimization (smaller splits for rapid iteration) and interpretability (evaluate different input types separately).\\nLearn how to create and manage dataset splits.\\nVersions\\nLangSmith automatically creates dataset versions when examples change. Tag versions to mark important milestones. Target specific versions in CI pipelines to ensure dataset updates don’t break workflows.\\n\\u200bHuman feedback collection\\nHuman feedback often provides the most valuable assessment, particularly for subjective quality dimensions.\\nAnnotation queues'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='\\u200bHuman feedback collection\\nHuman feedback often provides the most valuable assessment, particularly for subjective quality dimensions.\\nAnnotation queues\\nAnnotation queues enable structured collection of human feedback. Flag specific runs for review, collect annotations in a streamlined interface, and transfer annotated runs to datasets for future evaluations.\\nAnnotation queues complement inline annotation by offering additional capabilities: grouping runs, specifying criteria, and configuring reviewer permissions.\\n\\u200bEvaluations vs testing\\nTesting and evaluation are similar but distinct concepts.\\nEvaluation measures performance according to metrics. Metrics can be fuzzy or subjective, and prove more useful in relative terms. They typically compare systems against each other.\\nTesting asserts correctness. A system can only be deployed if it passes all tests.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='Testing asserts correctness. A system can only be deployed if it passes all tests.\\nEvaluation metrics can be converted into tests. For example, regression tests can assert that new versions must outperform baseline versions on relevant metrics. Run tests and evaluations together for efficiency when systems are expensive to run.\\nEvaluations can be written using standard testing tools like pytest or Vitest/Jest.\\n\\u200bQuick reference: Offline vs online evaluation\\nThe following table summarizes the key differences between offline and online evaluations:'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='\\u200bQuick reference: Offline vs online evaluation\\nThe following table summarizes the key differences between offline and online evaluations:\\nOffline EvaluationOnline EvaluationRuns onDataset (Examples)Tracing Project (Runs/Threads)Data accessInputs, Outputs, Reference OutputsInputs, Outputs onlyWhen to usePre-deployment, during developmentProduction, post-deploymentPrimary use casesBenchmarking, unit testing, regression testing, backtestingReal-time monitoring, production feedback, anomaly detectionEvaluation timingBatch processing on curated test setsReal-time or near real-time on live trafficSetup locationEvaluation tab (SDK, UI, Prompt Playground)Observability tab (automated rules)Data requirementsRequires dataset curationNo dataset needed, evaluates live traces'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='Edit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoEvaluation quickstartPreviousApplication-specific evaluation approachesNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0277a7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2402ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vektorstoredb = FAISS.from_documents(document, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "137c15bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1a2abb3d000>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vektorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50f13aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLM-as-judge evaluators require careful review of scores and prompt tuning. Few-shot evaluators, which include examples of inputs, outputs, and expected grades in the grader prompt, often improve performance.\\nLearn about how to define an LLM-as-a-judge evaluator.\\n\\u200bPairwise\\nPairwise evaluators compare outputs from two application versions using heuristics (e.g., which response is longer), LLMs (with pairwise prompts), or human reviewers.\\nPairwise evaluation works well when directly scoring an output is difficult but comparing two outputs is straightforward. For example, in summarization tasks, choosing the more informative of two summaries is often easier than assigning an absolute score to a single summary.\\nLearn how run pairwise evaluations.\\n\\u200bReference-free vs reference-based evaluators\\nUnderstanding whether an evaluator requires reference outputs is essential for determining when it can be used.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Query From a vector db\n",
    "query=\"LLM outputs are non-deterministic, which makes response quality hard to assess.\"\n",
    "result=vektorstoredb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ea1b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16e6643d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x000001A2ABB3D6F0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001A2ABB3ED10>, root_client=<openai.OpenAI object at 0x000001A2ABB3D3C0>, root_async_client=<openai.AsyncOpenAI object at 0x000001A2ABB3D4E0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrieval Chain, Document chain\n",
    "\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd33bff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is LangSmith Evaluation used for? \\n\\nLangSmith Evaluation is used for measuring quality throughout the application lifecycle, from pre-deployment testing to production monitoring.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\":\"What is Evaluation concepts?\",\n",
    "    \"context\":[Document(page_content=\"LLM outputs are non-deterministic, which makes response quality hard to assess. Evaluations (evals) are a way to breakdown what “good” looks like and measure it. LangSmith Evaluation provides a framework for measuring quality throughout the application lifecycle, from pre-deployment testing to production monitoring. \")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49fb1a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1a2abb3d000>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Input--->Retrieval ---> Document Chain ---> LLM ---> Output\n",
    "\n",
    "vektorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "404a0f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vektorstoredb.as_retriever()\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1a5b7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001A2ABB3D000>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x000001A2ABB3D6F0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001A2ABB3ED10>, root_client=<openai.OpenAI object at 0x000001A2ABB3D3C0>, root_async_client=<openai.AsyncOpenAI object at 0x000001A2ABB3D4E0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c311476c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the primary purpose of evaluations in the context of LangSmith Evaluation? \\n\\nThe primary purpose of evaluations in the context of LangSmith Evaluation is to breakdown what \"good\" looks like and measure the quality of systems throughout the application lifecycle, from pre-deployment testing to production monitoring. Evaluations help in assessing response quality, especially in the presence of non-deterministic outputs.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=retrieval_chain.invoke({\n",
    "    \"input\":\"What is Evaluation concepts?\"\n",
    "})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30da7b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is Evaluation concepts?',\n",
       " 'context': [Document(id='dbb2d00a-5dc7-409d-a3bd-f3a634af1a5a', metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='Evaluation concepts - Docs by LangChainSkip to main contentDocs by LangChain home pageLangSmithSearch...⌘KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationEvaluation conceptsGet startedObservabilityEvaluationPrompt engineeringDeploymentPlatform setupReferenceOverviewQuickstartConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsCustom output renderingSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOn this pageWhat to evaluateOffline and online evaluationsOffline evaluationsOnline'),\n",
       "  Document(id='40cf2510-9316-462f-9cb6-27fa0b5a0064', metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='are non-deterministic, which makes response quality hard to assess. Evaluations (evals) are a way to breakdown what “good” looks like and measure it. LangSmith Evaluation provides a framework for measuring quality throughout the application lifecycle, from pre-deployment testing to production monitoring.'),\n",
       "  Document(id='cb7d9e96-46d1-45f2-a1e2-51fccfe46726', metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOn this pageWhat to evaluateOffline and online evaluationsOffline evaluationsOnline evaluationsEvaluation lifecycle1. Development with offline evaluation2. Initial deployment with online evaluation3. Continuous improvementCore evaluation targetsTargets for offline evaluationDatasetsExamplesExperimentTargets for online evaluationRunsThreadsEvaluatorsEvaluator inputsEvaluator outputsEvaluation techniquesHumanCodeLLM-as-judgePairwiseReference-free vs reference-based evaluatorsEvaluation typesBest practicesBuilding datasetsDataset organizationHuman feedback collectionEvaluations vs testingQuick reference: Offline vs online evaluationEvaluation conceptsCopy pageCopy pageLLM outputs are non-deterministic, which makes response quality hard to assess. Evaluations (evals) are a way to breakdown what “good” looks like and measure it. LangSmith Evaluation provides a framework for'),\n",
       "  Document(id='78d24ec9-4dd9-4b3b-92df-12f697fe9637', metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='Testing asserts correctness. A system can only be deployed if it passes all tests.\\nEvaluation metrics can be converted into tests. For example, regression tests can assert that new versions must outperform baseline versions on relevant metrics. Run tests and evaluations together for efficiency when systems are expensive to run.\\nEvaluations can be written using standard testing tools like pytest or Vitest/Jest.\\n\\u200bQuick reference: Offline vs online evaluation\\nThe following table summarizes the key differences between offline and online evaluations:')],\n",
       " 'answer': 'What is the primary purpose of evaluations in the context of LangSmith Evaluation? \\n\\nThe primary purpose of evaluations in the context of LangSmith Evaluation is to breakdown what \"good\" looks like and measure the quality of systems throughout the application lifecycle, from pre-deployment testing to production monitoring. Evaluations help in assessing response quality, especially in the presence of non-deterministic outputs.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59676f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='dbb2d00a-5dc7-409d-a3bd-f3a634af1a5a', metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='Evaluation concepts - Docs by LangChainSkip to main contentDocs by LangChain home pageLangSmithSearch...⌘KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationEvaluation conceptsGet startedObservabilityEvaluationPrompt engineeringDeploymentPlatform setupReferenceOverviewQuickstartConceptsEvaluation approachesDatasetsCreate a datasetManage datasetsCustom output renderingSet up evaluationsRun an evaluationEvaluation typesFrameworks & integrationsEvaluation techniquesImprove evaluatorsTutorialsAnalyze experiment resultsAnalyze an experimentCompare experiment resultsFilter experiments in the UIFetch performance metrics for an experimentUpload experiments run outside of LangSmithAnnotation & human feedbackUse annotation queuesSet up feedback criteriaAnnotate traces and runs inlineAudit evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOn this pageWhat to evaluateOffline and online evaluationsOffline evaluationsOnline'),\n",
       " Document(id='40cf2510-9316-462f-9cb6-27fa0b5a0064', metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='are non-deterministic, which makes response quality hard to assess. Evaluations (evals) are a way to breakdown what “good” looks like and measure it. LangSmith Evaluation provides a framework for measuring quality throughout the application lifecycle, from pre-deployment testing to production monitoring.'),\n",
       " Document(id='cb7d9e96-46d1-45f2-a1e2-51fccfe46726', metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='evaluator scoresCommon data typesExample data formatDataset prebuilt JSON schema typesDataset transformationsOn this pageWhat to evaluateOffline and online evaluationsOffline evaluationsOnline evaluationsEvaluation lifecycle1. Development with offline evaluation2. Initial deployment with online evaluation3. Continuous improvementCore evaluation targetsTargets for offline evaluationDatasetsExamplesExperimentTargets for online evaluationRunsThreadsEvaluatorsEvaluator inputsEvaluator outputsEvaluation techniquesHumanCodeLLM-as-judgePairwiseReference-free vs reference-based evaluatorsEvaluation typesBest practicesBuilding datasetsDataset organizationHuman feedback collectionEvaluations vs testingQuick reference: Offline vs online evaluationEvaluation conceptsCopy pageCopy pageLLM outputs are non-deterministic, which makes response quality hard to assess. Evaluations (evals) are a way to breakdown what “good” looks like and measure it. LangSmith Evaluation provides a framework for'),\n",
       " Document(id='78d24ec9-4dd9-4b3b-92df-12f697fe9637', metadata={'source': 'https://docs.langchain.com/langsmith/evaluation-concepts', 'title': 'Evaluation concepts - Docs by LangChain', 'language': 'en'}, page_content='Testing asserts correctness. A system can only be deployed if it passes all tests.\\nEvaluation metrics can be converted into tests. For example, regression tests can assert that new versions must outperform baseline versions on relevant metrics. Run tests and evaluations together for efficiency when systems are expensive to run.\\nEvaluations can be written using standard testing tools like pytest or Vitest/Jest.\\n\\u200bQuick reference: Offline vs online evaluation\\nThe following table summarizes the key differences between offline and online evaluations:')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
